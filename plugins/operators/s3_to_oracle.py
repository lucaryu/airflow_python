from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.oracle.hooks.oracle import OracleHook
import pandas as pd
import pendulum
import io
import oracledb
import pyarrow.parquet as pq
import gc
import time

class S3ParquetToOracleOperator(BaseOperator):
    """
    [Custom Operator]
    MinIOì˜ Parquet íŒŒì¼ì„ ê¸°ê°„(From-To)ë§Œí¼ ì½ì–´ Oracleì— ì ì¬
    íŠ¹ì§•: ì „ì²˜ë¦¬ ë¡œì§ ë‚´ì¥ (DPY-3013 ì—ëŸ¬ ë°©ì§€)
    """
    
    # DAGì—ì„œ {{ params.from_date }} ê°’ì„ ë°›ì„ ë³€ìˆ˜ ì§€ì •
    template_fields = ('from_date', 'to_date', 'bucket_name')

    @apply_defaults
    def __init__(
        self,
        oracle_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date,
        to_date,
        key_prefix='taxi',
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.oracle_conn_id = oracle_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.batch_size = batch_size

    def _get_oracle_conn(self):
        oracle_hook = OracleHook(oracle_conn_id=self.oracle_conn_id)
        conn_info = oracle_hook.get_connection(self.oracle_conn_id)
        service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
        dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
        
        conn = oracledb.connect(
            user=conn_info.login,
            password=conn_info.password,
            dsn=dsn
        )
        return conn

    def _preprocess_data(self, df):
        """
        [ì „ì²˜ë¦¬ í•µì‹¬]
        Oracle ì ì¬ ì „ íƒ€ì… ì—ëŸ¬(DPY-3013) ë°©ì§€ë¥¼ ìœ„í•´
        ë¬¸ìì—´ ì»¬ëŸ¼ì˜ NULLì„ 'N'ìœ¼ë¡œ, ìˆ«ìëŠ” 0ìœ¼ë¡œ ë³€í™˜
        """
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì •ì˜
        str_cols = ['STORE_AND_FWD_FLAG', 'VENDOR_ID', 'RATE_CODE_ID', 
                    'PAYMENT_TYPE', 'PULOCATION_ID', 'DOLOCATION_ID']
        
        for col in str_cols:
            if col in df.columns:
                # NULL -> 'N', ê·¸ë¦¬ê³  ê°•ì œ ë¬¸ìì—´ ë³€í™˜
                df[col] = df[col].fillna('N').astype(str).str.strip()

        # ë‚˜ë¨¸ì§€ëŠ” ìˆ«ìí˜•ìœ¼ë¡œ ê°€ì •í•˜ê³  NULL -> 0 ì²˜ë¦¬
        df = df.fillna(0)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        date_cols = ['TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME']
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        return df

    def execute(self, context):
        self.log.info(f"ğŸš€ [Custom Operator] ì‹œì‘: {self.from_date} ~ {self.to_date}")
        
        conn = self._get_oracle_conn()
        cursor = conn.cursor()

        try:
            # ì…ë ¥ë°›ì€ YYYYMMDD ë¬¸ìì—´ì„ ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜
            # (ì˜ˆ: '20230101' -> datetime object)
            try:
                start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
            except ValueError:
                # í˜¹ì‹œ YYYY-MM-DD í˜•ì‹ì´ ë“¤ì–´ì˜¤ë©´ ìë™ ì²˜ë¦¬
                start_dt = pendulum.parse(str(self.from_date))
                end_dt = pendulum.parse(str(self.to_date))

            current_dt = start_dt
            s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

            # ì›” ë‹¨ìœ„ ë°˜ë³µ
            while current_dt <= end_dt:
                year = current_dt.format('YYYY')
                month = current_dt.format('MM')
                
                file_key = f"{self.key_prefix}/year={year}/month={month}/yellow_tripdata_{year}-{month}.parquet"
                self.log.info(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_key}")
                
                file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
                
                if not file_obj:
                    self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")
                    current_dt = current_dt.add(months=1)
                    continue

                # íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°
                data_stream = io.BytesIO(file_obj.get()['Body'].read())
                parquet_file = pq.ParquetFile(data_stream)

                target_columns = [
                    'VENDOR_ID', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME', 
                    'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATE_CODE_ID', 
                    'STORE_AND_FWD_FLAG', 'PULOCATION_ID', 'DOLOCATION_ID', 
                    'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX', 
                    'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE', 
                    'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE'
                ]
                
                insert_sql = f"""
                INSERT INTO {self.target_table} ({', '.join(target_columns)}) 
                VALUES ({', '.join([':' + str(i+1) for i in range(len(target_columns))])})
                """

                total_rows = 0
                for batch in parquet_file.iter_batches(batch_size=self.batch_size):
                    df_chunk = batch.to_pandas()
                    
                    # ì»¬ëŸ¼ ì´ë¦„ ë§¤í•‘
                    df_chunk = df_chunk.rename(columns={
                        'VendorID': 'VENDOR_ID', 'tpep_pickup_datetime': 'TPEP_PICKUP_DATETIME',
                        'tpep_dropoff_datetime': 'TPEP_DROPOFF_DATETIME', 'passenger_count': 'PASSENGER_COUNT',
                        'trip_distance': 'TRIP_DISTANCE', 'RatecodeID': 'RATE_CODE_ID',
                        'store_and_fwd_flag': 'STORE_AND_FWD_FLAG', 'PULocationID': 'PULOCATION_ID',
                        'DOLocationID': 'DOLOCATION_ID', 'payment_type': 'PAYMENT_TYPE',
                        'fare_amount': 'FARE_AMOUNT', 'extra': 'EXTRA', 'mta_tax': 'MTA_TAX',
                        'tip_amount': 'TIP_AMOUNT', 'tolls_amount': 'TOLLS_AMOUNT',
                        'improvement_surcharge': 'IMPROVEMENT_SURCHARGE', 'total_amount': 'TOTAL_AMOUNT',
                        'congestion_surcharge': 'CONGESTION_SURCHARGE', 'airport_fee': 'AIRPORT_FEE'
                    })
                    
                    # ì—†ëŠ” ì»¬ëŸ¼ ì±„ìš°ê¸°
                    for col in target_columns:
                        if col not in df_chunk.columns: df_chunk[col] = None
                    
                    # [ì „ì²˜ë¦¬ ìˆ˜í–‰]
                    df_chunk = self._preprocess_data(df_chunk)
                    
                    # ë°ì´í„° ì¤€ë¹„
                    df_chunk = df_chunk[target_columns]
                    rows = [tuple(x) for x in df_chunk.to_numpy()]
                    
                    cursor.executemany(insert_sql, rows)
                    total_rows += len(rows)
                    
                    del df_chunk, rows
                    gc.collect()

                conn.commit()
                self.log.info(f"âœ… {year}-{month} ë°ì´í„° {total_rows}ê±´ ì ì¬ ì™„ë£Œ")
                
                # ë‹¤ìŒ ë‹¬ë¡œ ì´ë™
                current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()