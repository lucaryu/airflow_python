from airflow.models import BaseOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.oracle.hooks.oracle import OracleHook
import pandas as pd
import pendulum
import io
import oracledb
import pyarrow.parquet as pq
import gc
import time

class S3ParquetToOracleOperator(BaseOperator):
    """
    [Custom Operator]
    S3(MinIO)ì˜ Parquet íŒŒì¼ì„ ê¸°ê°„(From-To)ë§Œí¼ ì½ì–´ Oracleì— ì ì¬
    íŠ¹ì§•: ì „ì²˜ë¦¬ ë¡œì§ ë‚´ì¥ (ORA-01722 ë° DPY-3013 ë°©ì§€)
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'target_table')

    def __init__(
        self,
        oracle_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date,
        to_date,
        key_prefix='taxi',
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.oracle_conn_id = oracle_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.batch_size = batch_size

    def _get_oracle_conn(self):
        oracle_hook = OracleHook(oracle_conn_id=self.oracle_conn_id)
        conn_info = oracle_hook.get_connection(self.oracle_conn_id)
        service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
        dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
        
        conn = oracledb.connect(
            user=conn_info.login,
            password=conn_info.password,
            dsn=dsn
        )
        return conn

    def _preprocess_data(self, df):
        """
        [ì „ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •]
        - STORE_AND_FWD_FLAG: ë¬¸ìì—´(VARCHAR)ì´ë¯€ë¡œ NULL -> 'N'
        - VENDOR_ID ë“± ID ì»¬ëŸ¼: ìˆ«ìí˜•(NUMBER)ì´ë¯€ë¡œ NULL -> 0 (ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°í•¨)
        """
        
        # â–¼ [ìˆ˜ì •] ì§„ì§œ ë¬¸ìì—´ ì»¬ëŸ¼ë§Œ ë‚¨ê¹ë‹ˆë‹¤. (ë‚˜ë¨¸ì§€ëŠ” ìë™ìœ¼ë¡œ 0 ì²˜ë¦¬ë¨)
        str_cols = ['STORE_AND_FWD_FLAG']
        
        for col in str_cols:
            if col in df.columns:
                # NULL -> 'N', ê·¸ë¦¬ê³  ê°•ì œ ë¬¸ìì—´ ë³€í™˜
                df[col] = df[col].fillna('N').astype(str).str.strip()

        # ë‚˜ë¨¸ì§€ëŠ” ìˆ«ìí˜•ìœ¼ë¡œ ê°€ì •í•˜ê³  NULL -> 0 ì²˜ë¦¬
        # (VENDOR_ID, PULOCATION_ID ë“±ì´ ì—¬ê¸°ì„œ 0ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ORA-01722 ë°©ì§€)
        df = df.fillna(0)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        date_cols = ['TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME']
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        return df

    def execute(self, context):
        self.log.info(f"ğŸš€ [S3ParquetToOracleOperator] ì‹œì‘: {self.from_date} ~ {self.to_date}")
        self.log.info(f"ğŸ¯ íƒ€ê²Ÿ í…Œì´ë¸”: {self.target_table}")
        
        conn = self._get_oracle_conn()
        cursor = conn.cursor()

        try:
            try:
                start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
            except ValueError:
                start_dt = pendulum.parse(str(self.from_date))
                end_dt = pendulum.parse(str(self.to_date))

            current_dt = start_dt
            s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

            while current_dt <= end_dt:
                year = current_dt.format('YYYY')
                month = current_dt.format('MM')
                
                file_key = f"{self.key_prefix}/year={year}/month={month}/yellow_tripdata_{year}-{month}.parquet"
                self.log.info(f"ğŸ“‚ íŒŒì¼ íƒìƒ‰: {file_key}")
                
                file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
                
                if not file_obj:
                    self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")
                    current_dt = current_dt.add(months=1)
                    continue

                data_stream = io.BytesIO(file_obj.get()['Body'].read())
                parquet_file = pq.ParquetFile(data_stream)

                target_columns = [
                    'VENDOR_ID', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME', 
                    'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATE_CODE_ID', 
                    'STORE_AND_FWD_FLAG', 'PULOCATION_ID', 'DOLOCATION_ID', 
                    'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX', 
                    'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE', 
                    'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE'
                ]
                
                insert_sql = f"""
                INSERT INTO {self.target_table} ({', '.join(target_columns)}) 
                VALUES ({', '.join([':' + str(i+1) for i in range(len(target_columns))])})
                """

                total_rows = 0
                for batch in parquet_file.iter_batches(batch_size=self.batch_size):
                    df_chunk = batch.to_pandas()
                    
                    df_chunk = df_chunk.rename(columns={
                        'VendorID': 'VENDOR_ID', 'tpep_pickup_datetime': 'TPEP_PICKUP_DATETIME',
                        'tpep_dropoff_datetime': 'TPEP_DROPOFF_DATETIME', 'passenger_count': 'PASSENGER_COUNT',
                        'trip_distance': 'TRIP_DISTANCE', 'RatecodeID': 'RATE_CODE_ID',
                        'store_and_fwd_flag': 'STORE_AND_FWD_FLAG', 'PULocationID': 'PULOCATION_ID',
                        'DOLocationID': 'DOLOCATION_ID', 'payment_type': 'PAYMENT_TYPE',
                        'fare_amount': 'FARE_AMOUNT', 'extra': 'EXTRA', 'mta_tax': 'MTA_TAX',
                        'tip_amount': 'TIP_AMOUNT', 'tolls_amount': 'TOLLS_AMOUNT',
                        'improvement_surcharge': 'IMPROVEMENT_SURCHARGE', 'total_amount': 'TOTAL_AMOUNT',
                        'congestion_surcharge': 'CONGESTION_SURCHARGE', 'airport_fee': 'AIRPORT_FEE'
                    })
                    
                    for col in target_columns:
                        if col not in df_chunk.columns: df_chunk[col] = None
                    
                    df_chunk = self._preprocess_data(df_chunk)
                    
                    df_chunk = df_chunk[target_columns]
                    rows = [tuple(x) for x in df_chunk.to_numpy()]
                    
                    cursor.executemany(insert_sql, rows)
                    total_rows += len(rows)
                    
                    del df_chunk, rows
                    gc.collect()

                conn.commit()
                self.log.info(f"âœ… {year}-{month} ì²˜ë¦¬ ì™„ë£Œ: {total_rows}ê±´ ì ì¬ë¨")
                
                current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()