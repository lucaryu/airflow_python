from airflow.models import BaseOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
import pendulum
import io
import pyarrow.parquet as pq
import gc

class S3ParquetToPostgresOperator(BaseOperator):
    """
    [Universal Custom Operator]
    S3(MinIO) -> PostgreSQL ì´ˆê³ ì† ì ì¬ (COPY ëª…ë ¹ ì‚¬ìš©)
    - date_column íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´: í•´ë‹¹ ê¸°ê°„ ë°ì´í„°ë§Œ ì‚­ì œ í›„ ì ì¬ (Incremental)
    - date_column íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´: í…Œì´ë¸” ì „ì²´ ë¹„ìš°ê³ (TRUNCATE) ì ì¬ (Full Load)
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'target_table', 'key_prefix', 'date_column')

    def __init__(
        self,
        postgres_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date,
        to_date,
        key_prefix='taxi',
        date_column=None,  # [ì¶”ê°€] ë‚ ì§œ ê¸°ì¤€ ì»¬ëŸ¼ (ì—†ìœ¼ë©´ Full Load)
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.postgres_conn_id = postgres_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.date_column = date_column
        self.batch_size = batch_size

    def _get_postgres_conn(self):
        pg_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        return pg_hook.get_conn()

    def _preprocess_data(self, df):
        # ë¬¸ìì—´ ì»¬ëŸ¼ ì²˜ë¦¬
        str_cols = ['STORE_AND_FWD_FLAG', 'VENDOR_ID', 'RATE_CODE_ID', 
                    'PAYMENT_TYPE', 'PULOCATION_ID', 'DOLOCATION_ID']
        for col in str_cols:
            if col in df.columns:
                df[col] = df[col].fillna('N').astype(str).str.strip()
        
        df = df.fillna(0)
        
        # ì •ìˆ˜í˜• ë³€í™˜ì´ í•„ìš”í•œ ì»¬ëŸ¼ë“¤ (ì—ëŸ¬ ë°©ì§€)
        int_cols = ['PASSENGER_COUNT']
        for col in int_cols:
            if col in df.columns:
                df[col] = df[col].astype(int)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
        date_cols = ['TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME']
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        return df

    def execute(self, context):
        self.log.info(f"ğŸš€ [S3ParquetToPostgresOperator] COPY ì ì¬ ì‹œì‘: {self.from_date} ~ {self.to_date}")
        
        conn = self._get_postgres_conn()
        cursor = conn.cursor()

        try:
            try:
                start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
            except ValueError:
                start_dt = pendulum.parse(str(self.from_date))
                end_dt = pendulum.parse(str(self.to_date))

            # ---------------------------------------------------------
            # [Full Load ì²˜ë¦¬] ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¨¼ì € í…Œì´ë¸”ì„ ì‹¹ ë¹„ìš´ë‹¤
            # ---------------------------------------------------------
            if not self.date_column:
                self.log.info(f"ğŸ§¹ Full Load ëª¨ë“œ: í…Œì´ë¸”({self.target_table}) ì „ì²´ ë¹„ìš°ê¸° (TRUNCATE)")
                cursor.execute(f"TRUNCATE TABLE {self.target_table}")
                conn.commit()

            current_dt = start_dt
            s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

            while current_dt <= end_dt:
                year = current_dt.format('YYYY')
                month = current_dt.format('MM')
                
                # ---------------------------------------------------------
                # [Incremental Load ì²˜ë¦¬] ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì›” ë°ì´í„°ë§Œ ì‚­ì œ
                # ---------------------------------------------------------
                if self.date_column:
                    next_month = current_dt.add(months=1).format('YYYY-MM-01')
                    current_month_start = current_dt.format('YYYY-MM-01')
                    
                    delete_sql = f"""
                        DELETE FROM {self.target_table} 
                        WHERE {self.date_column} >= '{current_month_start}' 
                          AND {self.date_column} < '{next_month}'
                    """
                    self.log.info(f"ğŸ§¹ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘... ({year}-{month})")
                    cursor.execute(delete_sql)

                # íŒŒì¼ ì´ë¦„ ê·œì¹™ (OracleToS3ì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤)
                filename = f"yellow_tripdata_{year}-{month}.parquet"
                file_key = f"{self.key_prefix}/year={year}/month={month}/{filename}"
                
                self.log.info(f"ğŸ“‚ íŒŒì¼ íƒìƒ‰: {file_key}")
                
                if not s3_hook.check_for_key(file_key, bucket_name=self.bucket_name):
                    self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")
                    current_dt = current_dt.add(months=1)
                    continue

                # S3 íŒŒì¼ ì½ê¸° ë° COPY ì ì¬
                file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
                data_stream = io.BytesIO(file_obj.get()['Body'].read())
                parquet_file = pq.ParquetFile(data_stream)
                
                # Parquet íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ COPY (ìˆœì„œ ì¤‘ìš”)
                # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì½ì–´ì„œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                schema = parquet_file.schema.names
                target_columns = schema # Parquet ì»¬ëŸ¼ ìˆœì„œëŒ€ë¡œ DBì— ë„£ìŒ
                
                total_rows = 0
                for batch in parquet_file.iter_batches(batch_size=self.batch_size):
                    df_chunk = batch.to_pandas()
                    
                    # ì „ì²˜ë¦¬ (NULL ì²˜ë¦¬, íƒ€ì… ë³€í™˜ ë“±)
                    df_chunk = self._preprocess_data(df_chunk)
                    
                    # ë©”ëª¨ë¦¬ ë‚´ CSV ë³€í™˜
                    csv_buffer = io.StringIO()
                    df_chunk.to_csv(csv_buffer, index=False, header=False, sep='\t', na_rep='\\N')
                    csv_buffer.seek(0)
                    
                    cursor.copy_expert(
                        f"COPY {self.target_table} ({', '.join(target_columns)}) FROM STDIN", 
                        csv_buffer
                    )
                    
                    total_rows += len(df_chunk)
                    
                    del df_chunk, csv_buffer
                    gc.collect()

                conn.commit()
                self.log.info(f"âœ… {year}-{month} COPY ì™„ë£Œ: {total_rows}ê±´ ì ì¬ë¨")
                
                current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()