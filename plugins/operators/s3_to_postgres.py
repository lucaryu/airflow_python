from airflow.models import BaseOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
import pendulum
import io
import pyarrow.parquet as pq
import gc

class S3ParquetToPostgresOperator(BaseOperator):
    """
    [Smart Loader Operator]
    S3 -> Postgres ì ì¬
    1. from_date/to_dateê°€ ì—†ìœ¼ë©´ (Full Load):
       - í…Œì´ë¸” TRUNCATE (ì „ì²´ ì‚­ì œ)
       - S3 í´ë” ë‚´ ëª¨ë“  Parquet íŒŒì¼ ì ì¬
    2. from_date/to_dateê°€ ìˆìœ¼ë©´ (Incremental Load):
       - date_columnì´ ìˆìœ¼ë©´ í•´ë‹¹ ê¸°ê°„ ë°ì´í„° DELETE (ë¶€ë¶„ ì‚­ì œ)
       - í•´ë‹¹ ê¸°ê°„ì˜ S3 íŒŒì¼ë§Œ ì ì¬
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'target_table', 'key_prefix', 'date_column')

    def __init__(
        self,
        postgres_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date=None,
        to_date=None,
        key_prefix='taxi',
        date_column=None,
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.postgres_conn_id = postgres_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.date_column = date_column
        self.batch_size = batch_size

    def _get_postgres_conn(self):
        pg_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        return pg_hook.get_conn()

    def _preprocess_data(self, df):
        # 1. ë¬¸ìì—´ ì»¬ëŸ¼ ì²˜ë¦¬ (NULL -> 'N', ê³µë°± ì œê±°)
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = df[col].fillna('N').astype(str).str.strip()
        
        # 2. ìˆ«ìí˜• NULL ì²˜ë¦¬ (0ìœ¼ë¡œ ì±„ì›€)
        df = df.fillna(0)
        
        # 3. ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ ë° ë³€í™˜
        for col in df.columns:
            if 'TIME' in col.upper() or 'DATE' in col.upper():
                 df[col] = pd.to_datetime(df[col], errors='coerce')
        return df

    def execute(self, context):
        conn = self._get_postgres_conn()
        cursor = conn.cursor()
        s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

        try:
            # âœ… ë‚ ì§œ íŒŒë¼ë¯¸í„° ìœ ë¬´ í™•ì¸
            # Airflow paramì´ ë¹„ì–´ìˆìœ¼ë©´ None ë˜ëŠ” ë¹ˆ ë¬¸ìì—´('')ë¡œ ë“¤ì–´ì˜´
            has_date = (self.from_date and str(self.from_date).strip()) and \
                       (self.to_date and str(self.to_date).strip())

            # =========================================================
            # CASE 1: Full Load (ë‚ ì§œ ì—†ìŒ -> TRUNCATE -> ëª¨ë“  íŒŒì¼)
            # =========================================================
            if not has_date:
                self.log.info(f"ğŸ“¦ [Full Load] ë‚ ì§œ ë²”ìœ„ ì—†ìŒ -> í…Œì´ë¸”({self.target_table}) TRUNCATE ì‹¤í–‰")
                cursor.execute(f"TRUNCATE TABLE {self.target_table}")
                conn.commit()

                # S3 í•´ë‹¹ í´ë”(prefix) ë°‘ì˜ ëª¨ë“  íŒŒì¼ ì¡°íšŒ
                self.log.info(f"ğŸ“‚ S3 ì „ì²´ ìŠ¤ìº” ì¤‘: {self.key_prefix}/")
                all_objs = s3_hook.list_keys(bucket_name=self.bucket_name, prefix=self.key_prefix)
                
                # .parquet íŒŒì¼ë§Œ í•„í„°ë§
                target_files = [f for f in all_objs if f.endswith('.parquet')] if all_objs else []
                
                if not target_files:
                    self.log.warning("âš ï¸ ì ì¬í•  S3 íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
                    return

                self.log.info(f"ì´ {len(target_files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìˆœì°¨ ì ì¬ ì‹œì‘.")
                for file_key in target_files:
                    self._load_single_file(s3_hook, cursor, file_key, conn)

            # =========================================================
            # CASE 2: Incremental Load (ë‚ ì§œ ìˆìŒ -> DELETE -> ê¸°ê°„ íŒŒì¼)
            # =========================================================
            else:
                self.log.info(f"ğŸ”„ [Incremental Load] ê¸°ê°„: {self.from_date} ~ {self.to_date}")
                
                # ë‚ ì§œ íŒŒì‹±
                try:
                    start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                    end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
                except ValueError:
                    start_dt = pendulum.parse(str(self.from_date))
                    end_dt = pendulum.parse(str(self.to_date))

                current_dt = start_dt
                while current_dt <= end_dt:
                    year = current_dt.format('YYYY')
                    month = current_dt.format('MM')
                    
                    # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (date_columnì´ ìˆì„ ë•Œë§Œ)
                    if self.date_column:
                        next_month = current_dt.add(months=1).format('YYYY-MM-01')
                        current_month_start = current_dt.format('YYYY-MM-01')
                        
                        delete_sql = f"""
                            DELETE FROM {self.target_table} 
                            WHERE {self.date_column} >= '{current_month_start}' 
                              AND {self.date_column} < '{next_month}'
                        """
                        self.log.info(f"ğŸ§¹ ê¸°ê°„ ì‚­ì œ ì‹¤í–‰ ({year}-{month})")
                        cursor.execute(delete_sql)
                    else:
                        self.log.info(f"â„¹ï¸ date_column ì—†ìŒ -> ì‚­ì œ ê±´ë„ˆëœ€ ({year}-{month})")

                    # 2. í•´ë‹¹ ì›” íŒŒì¼ ì ì¬
                    filename = f"yellow_tripdata_{year}-{month}.parquet"
                    file_key = f"{self.key_prefix}/year={year}/month={month}/{filename}"
                    
                    if s3_hook.check_for_key(file_key, bucket_name=self.bucket_name):
                        self._load_single_file(s3_hook, cursor, file_key, conn)
                    else:
                        self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")

                    current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()

    def _load_single_file(self, s3_hook, cursor, file_key, conn):
        """íŒŒì¼ í•˜ë‚˜ë¥¼ COPY ëª…ë ¹ì–´ë¡œ ì ì¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        self.log.info(f"ğŸ“¥ ì ì¬ ì‹œì‘: {file_key}")
        
        file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
        data_stream = io.BytesIO(file_obj.get()['Body'].read())
        parquet_file = pq.ParquetFile(data_stream)
        
        target_columns = parquet_file.schema.names
        
        total_rows = 0
        for batch in parquet_file.iter_batches(batch_size=self.batch_size):
            df_chunk = batch.to_pandas()
            df_chunk = self._preprocess_data(df_chunk)
            
            csv_buffer = io.StringIO()
            df_chunk.to_csv(csv_buffer, index=False, header=False, sep='\t', na_rep='\\N')
            csv_buffer.seek(0)
            
            cursor.copy_expert(
                f"COPY {self.target_table} ({', '.join(target_columns)}) FROM STDIN", 
                csv_buffer
            )
            total_rows += len(df_chunk)
            del df_chunk, csv_buffer
            gc.collect()
            
        conn.commit()
        self.log.info(f"âœ… ì ì¬ ì™„ë£Œ: {total_rows}ê±´")