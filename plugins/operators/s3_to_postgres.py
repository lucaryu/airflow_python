from airflow.models import BaseOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
import pendulum
import io
import pyarrow.parquet as pq
import gc

class S3ParquetToPostgresOperator(BaseOperator):
    """
    [Smart Loader Operator]
    S3 -> Postgres ì ì¬
    - Full Load: {prefix}/{prefix}_full.{ext} ìµœì‹  íŒŒì¼ 1ê°œë§Œ ì ì¬
    - Incremental: {prefix}/{YYYY}/{YYYYMM}/{prefix}_{YYYYMM}.{ext} ì ì¬
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'target_table', 'key_prefix', 'date_column')

    def __init__(
        self,
        postgres_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date=None,
        to_date=None,
        key_prefix='taxi',
        date_column=None,
        file_extension='parquet',
        csv_delimiter=',',
        csv_has_header=True,
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.postgres_conn_id = postgres_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.date_column = date_column
        self.file_extension = file_extension
        self.csv_delimiter = csv_delimiter
        self.csv_has_header = csv_has_header
        self.batch_size = batch_size

    def _get_postgres_conn(self):
        pg_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        return pg_hook.get_conn()

    def _preprocess_data(self, df):
        """ë°ì´í„° íƒ€ì…ë³„ NULL ì²˜ë¦¬ ë° í˜•ë³€í™˜"""
        
        # ì»¬ëŸ¼ë“¤ì˜ ì‹¤ì œ ë°ì´í„° íƒ€ì…(Type) ë¶„ë¥˜
        num_cols = df.select_dtypes(include=['number']).columns
        obj_cols = df.select_dtypes(include=['object', 'string']).columns

        # 1. ë‚ ì§œ ì»¬ëŸ¼ ê°•ì œ ë³€í™˜
        date_keywords = ['DATE', 'TIME', 'SINCE', 'DT', 'TIMESTAMP', 'DAY']
        for col in df.columns:
            if any(k in col.upper() for k in date_keywords):
                # â–¼â–¼â–¼ [í•µì‹¬ ìˆ˜ì •] ìˆ«ìí˜•(int, float ë“±)ì´ê±°ë‚˜ ë¬¸ìì—´(object)ì´ë©´ ë‚ ì§œ ë³€í™˜ì—ì„œ ë¬´ì¡°ê±´ ì œì™¸ â–¼â–¼â–¼
                if col in num_cols or col in obj_cols:
                    continue 
                df[col] = pd.to_datetime(df[col], errors='coerce')

        # 2. ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ NULL -> 0 ë³€í™˜
        if len(num_cols) > 0:
            df[num_cols] = df[num_cols].fillna(0)
        
        # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì²˜ë¦¬ (NULL -> \N)
        if len(obj_cols) > 0:
            for col in obj_cols:
                df[col] = df[col].fillna('\\N').astype(str).str.strip()
        
        return df

    def execute(self, context):
        conn = self._get_postgres_conn()
        cursor = conn.cursor()
        s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

        try:
            def is_valid_date(d):
                return d and str(d).strip().lower() not in ['none', '', 'null']

            has_date = is_valid_date(self.from_date) and is_valid_date(self.to_date)

            # =========================================================
            # CASE 1: Full Load (í…Œì´ë¸”ëª…_full.parquet ì°¾ê¸°)
            # =========================================================
            if not has_date:
                self.log.info(f"ğŸ“¦ [Full Load] ë‚ ì§œ ë²”ìœ„ ì—†ìŒ -> í…Œì´ë¸”({self.target_table}) TRUNCATE ì‹¤í–‰")
                cursor.execute(f"TRUNCATE TABLE {self.target_table}")
                conn.commit()

                filename = f"{self.key_prefix}_full.{self.file_extension}"
                file_key = f"{self.key_prefix}/{filename}"
                
                self.log.info(f"ğŸ“‚ íŒŒì¼ íƒìƒ‰: {file_key}")
                
                if s3_hook.check_for_key(file_key, bucket_name=self.bucket_name):
                    self._load_single_file(s3_hook, cursor, file_key, conn)
                else:
                    self.log.warning(f"âš ï¸ Full Load íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_key}")

            # =========================================================
            # CASE 2: Incremental Load (YYYY/YYYYMM/í…Œì´ë¸”ëª…_YYYYMM.parquet ì°¾ê¸°)
            # =========================================================
            else:
                self.log.info(f"ğŸ”„ [Incremental Load] ê¸°ê°„: {self.from_date} ~ {self.to_date}")
                try:
                    start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                    end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
                except ValueError:
                    start_dt = pendulum.parse(str(self.from_date))
                    end_dt = pendulum.parse(str(self.to_date))

                current_dt = start_dt
                while current_dt <= end_dt:
                    year = current_dt.format('YYYY')
                    month = current_dt.format('MM')
                    yyyymm = current_dt.format('YYYYMM')
                    
                    if self.date_column and str(self.date_column).lower() != 'none':
                        next_month = current_dt.add(months=1).format('YYYY-MM-01')
                        current_month_start = current_dt.format('YYYY-MM-01')
                        
                        delete_sql = f"""
                            DELETE FROM {self.target_table} 
                            WHERE {self.date_column} >= '{current_month_start}' 
                              AND {self.date_column} < '{next_month}'
                        """
                        self.log.info(f"ğŸ§¹ ê¸°ê°„ ì‚­ì œ ì‹¤í–‰ ({year}-{month})")
                        cursor.execute(delete_sql)

                    filename = f"{self.key_prefix}_{yyyymm}.{self.file_extension}"
                    file_key = f"{self.key_prefix}/{year}/{yyyymm}/{filename}"
                    
                    if s3_hook.check_for_key(file_key, bucket_name=self.bucket_name):
                        self._load_single_file(s3_hook, cursor, file_key, conn)
                    else:
                        self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")

                    current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()

    def _load_single_file(self, s3_hook, cursor, file_key, conn):
        self.log.info(f"ğŸ“¥ ì ì¬ ì‹œì‘: {file_key}")
        
        file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
        data_stream = io.BytesIO(file_obj.get()['Body'].read())
        
        total_rows = 0
        if self.file_extension.lower() == 'parquet':
            parquet_file = pq.ParquetFile(data_stream)
            target_columns = parquet_file.schema.names
            
            for batch in parquet_file.iter_batches(batch_size=self.batch_size):
                df_chunk = batch.to_pandas()
                df_chunk = self._preprocess_data(df_chunk)
                
                csv_buffer = io.StringIO()
                df_chunk.to_csv(csv_buffer, index=False, header=False, sep='\t', na_rep='\\N')
                csv_buffer.seek(0)
                
                cursor.copy_expert(
                    f"COPY {self.target_table} ({', '.join(target_columns)}) FROM STDIN", 
                    csv_buffer
                )
                total_rows += len(df_chunk)
                del df_chunk, csv_buffer
                gc.collect()
        elif self.file_extension.lower() == 'csv':
            header_param = 'infer' if self.csv_has_header else None
            for df_chunk in pd.read_csv(data_stream, sep=self.csv_delimiter, header=header_param, chunksize=self.batch_size):
                
                # í—¤ë”ê°€ ì—†ëŠ” ê²½ìš° ì„ì˜ì˜ ì»¬ëŸ¼ëª…ì´ ìƒì„±ë˜ë¯€ë¡œ Postgres í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œëŒ€ë¡œ ë“¤ì–´ê°„ë‹¤ê³  ê°€ì •
                if not self.csv_has_header:
                    # Postgres í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ë“¤ì„ ì¡°íšŒí•´ì„œ ë§¤í•‘í•  ìˆ˜ë„ ìˆìœ¼ë‚˜,
                    # ì„±ëŠ¥ ë° ë³µì¡ì„±ì„ ìœ„í•´ ë‹¨ìˆœíˆ ìˆœí™˜í•˜ë©° DataFrameì˜ ì»¬ëŸ¼ ê°œìˆ˜ë§Œí¼ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                    target_columns = [f"col_{i}" for i in range(len(df_chunk.columns))]
                    df_chunk.columns = target_columns
                else:
                    target_columns = df_chunk.columns.tolist()
                    
                df_chunk = self._preprocess_data(df_chunk)
                
                csv_buffer = io.StringIO()
                df_chunk.to_csv(csv_buffer, index=False, header=False, sep='\t', na_rep='\\N')
                csv_buffer.seek(0)
                
                # í—¤ë”ê°€ ì—†ìœ¼ë©´ COPY ì‹œì— ëŒ€ìƒ ì»¬ëŸ¼ì„ ëª…ì‹œí•˜ì§€ ì•Šê±°ë‚˜, ìƒì„±ëœ ì»¬ëŸ¼ìœ¼ë¡œ ë³µì‚¬ë¨
                # ëŒ€ìƒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì™€ 1:1 ë§¤í•‘ëœë‹¤ê³  ê°€ì •
                if not self.csv_has_header:
                     cursor.copy_expert(
                        f"COPY {self.target_table} FROM STDIN", 
                        csv_buffer
                    )
                else:
                    cursor.copy_expert(
                        f"COPY {self.target_table} ({', '.join(target_columns)}) FROM STDIN", 
                        csv_buffer
                    )
                total_rows += len(df_chunk)
                del df_chunk, csv_buffer
                gc.collect()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ìì…ë‹ˆë‹¤: {self.file_extension}")
            
        conn.commit()
        self.log.info(f"âœ… ì ì¬ ì™„ë£Œ: {total_rows}ê±´")