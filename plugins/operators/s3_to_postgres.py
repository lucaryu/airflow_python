from airflow.models import BaseOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pandas as pd
import pendulum
import io
import pyarrow.parquet as pq
import gc

class S3ParquetToPostgresOperator(BaseOperator):
    """
    [Smart Loader Operator]
    S3 -> Postgres ì ì¬
    - ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ ê°•í™” (DATE, TIME, SINCE, DT ë“±)
    - NULL ì²˜ë¦¬ ê°œì„ : ìˆ«ìëŠ” 0, ë¬¸ìëŠ” 'N', ë‚ ì§œëŠ” NULL ìœ ì§€
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'target_table', 'key_prefix', 'date_column')

    def __init__(
        self,
        postgres_conn_id,
        minio_conn_id,
        target_table,
        bucket_name,
        from_date=None,
        to_date=None,
        key_prefix='taxi',
        date_column=None,
        batch_size=100000,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.postgres_conn_id = postgres_conn_id
        self.minio_conn_id = minio_conn_id
        self.target_table = target_table
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.key_prefix = key_prefix
        self.date_column = date_column
        self.batch_size = batch_size

    def _get_postgres_conn(self):
        pg_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)
        return pg_hook.get_conn()

    def _preprocess_data(self, df):
        """ë°ì´í„° íƒ€ì…ë³„ NULL ì²˜ë¦¬ ë° í˜•ë³€í™˜ (ë§¤ìš° ì¤‘ìš”!)"""
        
        # 1. ë‚ ì§œ ì»¬ëŸ¼ ê°•ì œ ë³€í™˜ (NULL ìœ ì§€ë¥¼ ìœ„í•´ ê°€ì¥ ë¨¼ì € ìˆ˜í–‰)
        # ì»¬ëŸ¼ëª…ì— ì•„ë˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë‚ ì§œë¡œ ì¸ì‹ (euro_in_use_since ëŒ€ì‘ì„ ìœ„í•´ 'SINCE' ì¶”ê°€)
        date_keywords = ['DATE', 'TIME', 'SINCE', 'DT', 'TIMESTAMP', 'DAY']
        
        for col in df.columns:
            if any(k in col.upper() for k in date_keywords):
                # errors='coerce'ëŠ” ë³€í™˜ ì‹¤íŒ¨(NULL í¬í•¨) ì‹œ NaT(Not a Time)ë¡œ ì„¤ì • -> DBì—ëŠ” NULLë¡œ ë“¤ì–´ê°
                df[col] = pd.to_datetime(df[col], errors='coerce')

        # 2. ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ NULL -> 0 ë³€í™˜
        # (ë‚ ì§œ ì»¬ëŸ¼ì€ ì´ë¯¸ datetime íƒ€ì…ì´ ë˜ì—ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ì œì™¸ë¨)
        num_cols = df.select_dtypes(include=['number']).columns
        df[num_cols] = df[num_cols].fillna(0)
        
        # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì²˜ë¦¬ (NULL -> ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” 'N')
        # object íƒ€ì… ì¤‘ datetimeì´ ì•„ë‹Œ ê²ƒë“¤
        obj_cols = df.select_dtypes(include=['object']).columns
        for col in obj_cols:
            df[col] = df[col].fillna('\\N').astype(str).str.strip()
            # ì£¼ì˜: Postgres COPYì—ì„œ \Nì€ NULLì„ ì˜ë¯¸í•¨. 
            # ë¹ˆê°’ìœ¼ë¡œ ë„£ê³  ì‹¶ìœ¼ë©´ '' ë¡œ ì„¤ì •. ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ \N(NULL) ì‚¬ìš© ê¶Œì¥.
            # ë§Œì•½ 'N' ë¬¸ìë¡œ ì±„ìš°ê³  ì‹¶ë‹¤ë©´ fillna('N') ì‚¬ìš©.
        
        return df

    def execute(self, context):
        conn = self._get_postgres_conn()
        cursor = conn.cursor()
        s3_hook = S3Hook(aws_conn_id=self.minio_conn_id)

        try:
            def is_valid_date(d):
                return d and str(d).strip().lower() not in ['none', '', 'null']

            has_date = is_valid_date(self.from_date) and is_valid_date(self.to_date)

            # =========================================================
            # CASE 1: Full Load
            # =========================================================
            if not has_date:
                self.log.info(f"ğŸ“¦ [Full Load] ë‚ ì§œ ë²”ìœ„ ì—†ìŒ -> í…Œì´ë¸”({self.target_table}) TRUNCATE ì‹¤í–‰")
                cursor.execute(f"TRUNCATE TABLE {self.target_table}")
                conn.commit()

                self.log.info(f"ğŸ“‚ S3 ì „ì²´ ìŠ¤ìº” ì¤‘: {self.key_prefix}/")
                all_objs = s3_hook.list_keys(bucket_name=self.bucket_name, prefix=self.key_prefix)
                target_files = [f for f in all_objs if f.endswith('.parquet')] if all_objs else []
                
                if not target_files:
                    self.log.warning("âš ï¸ ì ì¬í•  S3 íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
                    return

                self.log.info(f"ì´ {len(target_files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìˆœì°¨ ì ì¬ ì‹œì‘.")
                for file_key in target_files:
                    self._load_single_file(s3_hook, cursor, file_key, conn)

            # =========================================================
            # CASE 2: Incremental Load
            # =========================================================
            else:
                self.log.info(f"ğŸ”„ [Incremental Load] ê¸°ê°„: {self.from_date} ~ {self.to_date}")
                try:
                    start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                    end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
                except ValueError:
                    start_dt = pendulum.parse(str(self.from_date))
                    end_dt = pendulum.parse(str(self.to_date))

                current_dt = start_dt
                while current_dt <= end_dt:
                    year = current_dt.format('YYYY')
                    month = current_dt.format('MM')
                    
                    if self.date_column and str(self.date_column).lower() != 'none':
                        next_month = current_dt.add(months=1).format('YYYY-MM-01')
                        current_month_start = current_dt.format('YYYY-MM-01')
                        
                        delete_sql = f"""
                            DELETE FROM {self.target_table} 
                            WHERE {self.date_column} >= '{current_month_start}' 
                              AND {self.date_column} < '{next_month}'
                        """
                        self.log.info(f"ğŸ§¹ ê¸°ê°„ ì‚­ì œ ì‹¤í–‰ ({year}-{month})")
                        cursor.execute(delete_sql)

                    filename = f"yellow_tripdata_{year}-{month}.parquet"
                    file_key = f"{self.key_prefix}/year={year}/month={month}/{filename}"
                    
                    if s3_hook.check_for_key(file_key, bucket_name=self.bucket_name):
                        self._load_single_file(s3_hook, cursor, file_key, conn)
                    else:
                        self.log.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ (Skip): {file_key}")

                    current_dt = current_dt.add(months=1)

        except Exception as e:
            conn.rollback()
            self.log.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        finally:
            cursor.close()
            conn.close()

    def _load_single_file(self, s3_hook, cursor, file_key, conn):
        self.log.info(f"ğŸ“¥ ì ì¬ ì‹œì‘: {file_key}")
        
        file_obj = s3_hook.get_key(key=file_key, bucket_name=self.bucket_name)
        data_stream = io.BytesIO(file_obj.get()['Body'].read())
        parquet_file = pq.ParquetFile(data_stream)
        
        target_columns = parquet_file.schema.names
        
        total_rows = 0
        for batch in parquet_file.iter_batches(batch_size=self.batch_size):
            df_chunk = batch.to_pandas()
            
            # [ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ë¡œì§ ì‚¬ìš©]
            df_chunk = self._preprocess_data(df_chunk)
            
            csv_buffer = io.StringIO()
            # na_rep='\\N' -> Pandasì˜ NaT/NaN/Noneì„ Postgresì˜ NULL(\N)ë¡œ ë³€í™˜
            df_chunk.to_csv(csv_buffer, index=False, header=False, sep='\t', na_rep='\\N')
            csv_buffer.seek(0)
            
            cursor.copy_expert(
                f"COPY {self.target_table} ({', '.join(target_columns)}) FROM STDIN", 
                csv_buffer
            )
            total_rows += len(df_chunk)
            del df_chunk, csv_buffer
            gc.collect()
            
        conn.commit()
        self.log.info(f"âœ… ì ì¬ ì™„ë£Œ: {total_rows}ê±´")