from airflow.models import BaseOperator
from airflow.providers.oracle.hooks.oracle import OracleHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd
import pendulum
import io
import oracledb

class OracleToS3ParquetOperator(BaseOperator):
    """
    [Universal Custom Operator]
    Oracle SQL ê²°ê³¼ë¥¼ S3ì— Parquetë¡œ ì €ìž¥
    1. from_date/to_dateê°€ ì—†ìœ¼ë©´ (Full Load):
       - SQLì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰ (ì¹˜í™˜ ì—†ìŒ)
       - S3ì˜ 'full_load' í´ë”ì— ì €ìž¥
    2. from_date/to_dateê°€ ìžˆìœ¼ë©´ (Incremental Load):
       - SQL ë‚´ì˜ {start_date}, {end_date}ë¥¼ ì¹˜í™˜í•˜ë©° ë°˜ë³µ ì‹¤í–‰
       - S3ì˜ year=YYYY/month=MM í´ë”ì— ì €ìž¥
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'oracle_sql', 's3_key_prefix')

    def __init__(
        self,
        oracle_conn_id,
        s3_conn_id,
        oracle_sql,
        bucket_name,
        from_date=None,  # None í—ˆìš©
        to_date=None,    # None í—ˆìš©
        s3_key_prefix='taxi',
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.oracle_conn_id = oracle_conn_id
        self.s3_conn_id = s3_conn_id
        self.oracle_sql = oracle_sql
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.s3_key_prefix = s3_key_prefix

    def _get_oracle_conn(self):
        oracle_hook = OracleHook(oracle_conn_id=self.oracle_conn_id)
        conn_info = oracle_hook.get_connection(self.oracle_conn_id)
        service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
        dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
        return oracledb.connect(user=conn_info.login, password=conn_info.password, dsn=dsn)

    def execute(self, context):
        self.log.info(f"ðŸš€ [OracleToS3] ì‹œìž‘: {self.from_date} ~ {self.to_date}")

        # âœ… ë‚ ì§œ íŒŒë¼ë¯¸í„° ìœ ë¬´ í™•ì¸
        has_date = (self.from_date and str(self.from_date).strip() != 'None') and \
                   (self.to_date and str(self.to_date).strip() != 'None')

        oracle_conn = self._get_oracle_conn()
        s3_hook = S3Hook(aws_conn_id=self.s3_conn_id)

        try:
            # =========================================================
            # CASE 1: Full Load (ë‚ ì§œ ì—†ìŒ -> SQL ê·¸ëŒ€ë¡œ ì‹¤í–‰)
            # =========================================================
            if not has_date:
                self.log.info("ðŸ“¦ [Full Load] ë‚ ì§œ ë²”ìœ„ ì—†ìŒ -> SQL ê·¸ëŒ€ë¡œ ì‹¤í–‰")
                
                # ì¹˜í™˜ ì—†ì´ ì›ë³¸ SQL ì‹¤í–‰
                final_sql = self.oracle_sql
                
                # ì €ìž¥ ìœ„ì¹˜: full_load í´ë” (ì˜¤ëŠ˜ ë‚ ì§œ íŒŒì¼ëª…)
                today = pendulum.now('Asia/Seoul').format('YYYYMMDD')
                filename = f"full_load_{today}.parquet"
                s3_key = f"{self.s3_key_prefix}/full_load/{filename}"
                
                self._process_and_upload(oracle_conn, s3_hook, final_sql, s3_key)

            # =========================================================
            # CASE 2: Incremental Load (ë‚ ì§œ ìžˆìŒ -> ë°˜ë³µ ì‹¤í–‰)
            # =========================================================
            else:
                try:
                    start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
                    end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
                except ValueError:
                    start_dt = pendulum.parse(str(self.from_date))
                    end_dt = pendulum.parse(str(self.to_date))
                
                self.log.info("ðŸ”„ [Incremental Load] ì›”ë³„ ë¶„í•  ì¡°íšŒ ì‹œìž‘")
                
                current_dt = start_dt
                while current_dt <= end_dt:
                    year = current_dt.format('YYYY')
                    month = current_dt.format('MM')
                    
                    # ë‚ ì§œ ë³€ìˆ˜ ê³„ì‚°
                    current_month_str = current_dt.format('YYYY-MM-01')
                    next_month_str = current_dt.add(months=1).format('YYYY-MM-01')
                    
                    # SQL ì¹˜í™˜ ({start_date}, {end_date}ê°€ ìžˆì„ ê²½ìš°ë§Œ)
                    if "{start_date}" in self.oracle_sql:
                        final_sql = self.oracle_sql.format(
                            start_date=current_month_str,
                            end_date=next_month_str
                        )
                    else:
                        # ë‚ ì§œê°€ ìžˆëŠ”ë° SQLì— ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì‹¤í–‰ (ì¤‘ë³µ ì‹¤í–‰ ì£¼ì˜)
                        final_sql = self.oracle_sql
                    
                    # ì €ìž¥ ìœ„ì¹˜: ì—°/ì›” í´ë”
                    filename = f"yellow_tripdata_{year}-{month}.parquet"
                    s3_key = f"{self.s3_key_prefix}/year={year}/month={month}/{filename}"
                    
                    self._process_and_upload(oracle_conn, s3_hook, final_sql, s3_key)
                    current_dt = current_dt.add(months=1)

        finally:
            if oracle_conn:
                oracle_conn.close()

    def _process_and_upload(self, conn, s3_hook, sql, s3_key):
        self.log.info(f"ðŸ” ì¡°íšŒ ì‹¤í–‰...")
        self.log.debug(f"SQL: {sql}")
        
        df = pd.read_sql(sql, conn)
        
        if df.empty:
            self.log.warning(f"âš ï¸ ë°ì´í„° ì—†ìŒ (Skip)")
            return

        parquet_buffer = io.BytesIO()
        df.to_parquet(parquet_buffer, index=False, engine='pyarrow')
        parquet_buffer.seek(0)
        
        s3_hook.load_bytes(
            bytes_data=parquet_buffer.getvalue(),
            key=s3_key,
            bucket_name=self.bucket_name,
            replace=True
        )
        self.log.info(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key} ({len(df)}ê±´)")