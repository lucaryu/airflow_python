from airflow.models import BaseOperator
from airflow.providers.oracle.hooks.oracle import OracleHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd
import pendulum
import io
import oracledb

class OracleToS3ParquetOperator(BaseOperator):
    """
    [ä¸‡èƒ½ Custom Operator]
    Oracle -> S3 Parquet ì €ì¥
    - date_columnì´ ìˆìœ¼ë©´: ì›”ë³„ ë¶„í•  ì ì¬ (Incremental)
    - date_columnì´ ì—†ìœ¼ë©´: í•œ ë²ˆì— ì „ì²´ ì ì¬ (Full Load)
    """
    
    template_fields = ('from_date', 'to_date', 'bucket_name', 'oracle_sql', 'date_column')

    def __init__(
        self,
        oracle_conn_id,
        s3_conn_id,
        oracle_sql,       # ì‹¤í–‰í•  SQL (í…Œì´ë¸”ëª… ë˜ëŠ” ì¿¼ë¦¬)
        bucket_name,
        from_date,
        to_date,
        date_column=None, # [ì„ íƒ] ì—†ìœ¼ë©´ Full Load
        s3_key_prefix='taxi',
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.oracle_conn_id = oracle_conn_id
        self.s3_conn_id = s3_conn_id
        self.oracle_sql = oracle_sql
        self.bucket_name = bucket_name
        self.from_date = from_date
        self.to_date = to_date
        self.date_column = date_column # Noneì´ë©´ ì „ì²´ ì ì¬
        self.s3_key_prefix = s3_key_prefix

    def _get_oracle_conn(self):
        oracle_hook = OracleHook(oracle_conn_id=self.oracle_conn_id)
        conn_info = oracle_hook.get_connection(self.oracle_conn_id)
        service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
        dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
        return oracledb.connect(user=conn_info.login, password=conn_info.password, dsn=dsn)

    def execute(self, context):
        self.log.info(f"ğŸš€ [OracleToS3] ì‹œì‘: {self.from_date} ~ {self.to_date}")
        
        # ë‚ ì§œ íŒŒì‹±
        try:
            start_dt = pendulum.from_format(str(self.from_date), 'YYYYMMDD')
            end_dt = pendulum.from_format(str(self.to_date), 'YYYYMMDD')
        except ValueError:
            start_dt = pendulum.parse(str(self.from_date))
            end_dt = pendulum.parse(str(self.to_date))

        oracle_conn = self._get_oracle_conn()
        s3_hook = S3Hook(aws_conn_id=self.s3_conn_id)

        try:
            # ---------------------------------------------------------
            # CASE 1: ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° (ì›”ë³„ ë°˜ë³µ ì ì¬)
            # ---------------------------------------------------------
            if self.date_column and self.date_column.lower() != 'none' and self.date_column.strip() != '':
                self.log.info(f"ğŸ”„ ëª¨ë“œ: ì›”ë³„ ë¶„í•  ì ì¬ (ê¸°ì¤€ ì»¬ëŸ¼: {self.date_column})")
                
                current_dt = start_dt
                while current_dt <= end_dt:
                    year = current_dt.format('YYYY')
                    month = current_dt.format('MM')
                    
                    # ë‚ ì§œ ì¡°ê±´ ìƒì„±
                    next_month = current_dt.add(months=1).format('YYYY-MM-01')
                    current_month_str = current_dt.format('YYYY-MM-01')
                    
                    sql = f"""
                        SELECT * FROM ({self.oracle_sql}) 
                        WHERE {self.date_column} >= TO_DATE('{current_month_str}', 'YYYY-MM-DD')
                          AND {self.date_column} < TO_DATE('{next_month}', 'YYYY-MM-DD')
                    """
                    
                    self._process_and_upload(oracle_conn, s3_hook, sql, year, month)
                    current_dt = current_dt.add(months=1)

            # ---------------------------------------------------------
            # CASE 2: ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° (Full Load - í•œ ë²ˆë§Œ ì‹¤í–‰)
            # ---------------------------------------------------------
            else:
                self.log.info("ğŸ“¦ ëª¨ë“œ: ì „ì²´ í†µì ì¬ (Full Load)")
                
                # Full LoadëŠ” ë‚ ì§œ ì¡°ê±´ ì—†ì´ ì›ë³¸ SQL ê·¸ëŒ€ë¡œ ì‹¤í–‰
                sql = f"SELECT * FROM ({self.oracle_sql})"
                
                # ì €ì¥ ìœ„ì¹˜ëŠ” í¸ì˜ìƒ from_dateì˜ ì—°/ì›” í´ë”ì— ì €ì¥ (Loaderê°€ ì°¾ê¸° ì‰½ê²Œ)
                year = start_dt.format('YYYY')
                month = start_dt.format('MM')
                
                self._process_and_upload(oracle_conn, s3_hook, sql, year, month)

        finally:
            if oracle_conn:
                oracle_conn.close()

    def _process_and_upload(self, conn, s3_hook, sql, year, month):
        """ë°ì´í„° ì¡°íšŒ ë° S3 ì—…ë¡œë“œ ê³µí†µ í•¨ìˆ˜"""
        self.log.info(f"ğŸ” ì¡°íšŒ ì‹¤í–‰: {year}-{month}")
        df = pd.read_sql(sql, conn)
        
        if df.empty:
            self.log.warning(f"âš ï¸ ë°ì´í„° ì—†ìŒ (Skip): {year}-{month}")
            return

        parquet_buffer = io.BytesIO()
        df.to_parquet(parquet_buffer, index=False, engine='pyarrow')
        parquet_buffer.seek(0)
        
        # íŒŒì¼ëª… í†µì¼ (Loader í˜¸í™˜ì„± ìœ ì§€)
        filename = f"yellow_tripdata_{year}-{month}.parquet"
        s3_key = f"{self.s3_key_prefix}/year={year}/month={month}/{filename}"
        
        s3_hook.load_bytes(
            bytes_data=parquet_buffer.getvalue(),
            key=s3_key,
            bucket_name=self.bucket_name,
            replace=True
        )
        self.log.info(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key} ({len(df)}ê±´)")