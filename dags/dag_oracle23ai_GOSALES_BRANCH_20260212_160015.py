from airflow import DAG
from airflow.models.param import Param
from operators.oracle_to_s3 import OracleToS3ParquetOperator
from operators.s3_to_postgres import S3ParquetToPostgresOperator
import pendulum
from datetime import timedelta

# =========================================================
# ğŸ“ [ê°œë°œì ì˜ì—­] ì„¤ì •
# =========================================================

# 1. Oracle ì¡°íšŒ ì¿¼ë¦¬ (ë‚ ì§œ ë³€ìˆ˜ ì—†ìŒ -> ì „ì²´ ì¡°íšŒ)
SOURCE_SQL = """
    SELECT BRANCH_CODE
     , ADDRESS1
     , ADDRESS1_MB
     , ADDRESS2
     , ADDRESS2_MB
     , CITY
     , CITY_MB
     , PROV_STATE
     , PROV_STATE_MB
     , POSTAL_ZONE
     , COUNTRY_CODE
     , ORGANIZATION_CODE
     , WAREHOUSE_BRANCH_CODE
     , SYSDATE AS ETL_CRY_DTM
  FROM GOSALES.BRANCH
"""

# 2. ì ì¬ í…Œì´ë¸” ì´ë¦„
TARGET_TABLE = "branch"

# 3. ë‚ ì§œ ê¸°ì¤€ ì»¬ëŸ¼
# - ê°’ì´ ìˆìœ¼ë©´ (ì˜ˆ: 'REG_DATE'): ê¸°ê°„ë³„ DELETE í›„ ì ì¬
# - ê°’ì´ ì—†ìœ¼ë©´ (None): ì „ì²´ TRUNCATE í›„ ì ì¬ (ë‹¨, ë‚ ì§œ íŒŒë¼ë¯¸í„°ê°€ ì—†ì–´ì•¼ í•¨)
DATE_COLUMN = None  # ë§ˆìŠ¤í„° í…Œì´ë¸”ì´ë¯€ë¡œ None

# =========================================================

default_args = {
    'owner': 'airflow',
    'start_date': pendulum.datetime(2023, 1, 1, tz="Asia/Seoul"),
    'catchup': False,
    'execution_timeout': timedelta(hours=5)
}

# UI íŒŒë¼ë¯¸í„° ì„¤ì • (ë‚ ì§œë¥¼ ë¹„ìš¸ ìˆ˜ ìˆê²Œ typeì— null ì¶”ê°€)
params = {
    "from_date": Param(None, type=["string", "null"], description="ì‹œì‘ì¼ (ë¹„ìš°ë©´ Full Load)"),
    "to_date": Param(None, type=["string", "null"], description="ì¢…ë£Œì¼ (ë¹„ìš°ë©´ Full Load)"),
    "target_table": Param(TARGET_TABLE, type="string", description="Postgres ì ì¬ í…Œì´ë¸”ëª…")
}

with DAG(
    dag_id='dag_oracle23ai_GOSALES_BRANCH_20260212_160015',
    default_args=default_args,
    schedule=None,
    params=params,
    tags=['portfolio', 'oracle', 's3', 'postgres', 'hybrid_config'],
) as dag:

    # 1. Oracle -> S3
    extract_task = OracleToS3ParquetOperator(
        task_id='extract_oracle_to_s3',
        oracle_conn_id='oracle_conn',
        s3_conn_id='minio_conn',
        bucket_name='bronze',
        
        oracle_sql=SOURCE_SQL,
        
        from_date='{{ params.from_date }}',
        to_date='{{ params.to_date }}',
        s3_key_prefix='{{ params.target_table | lower }}'
    )

    # 2. S3 -> Postgres
    load_task = S3ParquetToPostgresOperator(
        task_id='load_s3_to_postgres',
        postgres_conn_id='postgres_default',
        minio_conn_id='minio_conn',
        bucket_name='bronze',
        
        target_table='{{ params.target_table }}',
        from_date='{{ params.from_date }}',
        to_date='{{ params.to_date }}',
        key_prefix='{{ params.target_table | lower }}',
        
        # â–¼ ìƒë‹¨ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
        date_column=DATE_COLUMN,
        
        batch_size=100000
    )

    extract_task >> load_task