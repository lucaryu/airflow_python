from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.oracle.hooks.oracle import OracleHook
import pandas as pd
import pendulum
import io
import oracledb
import pyarrow.parquet as pq
import gc
import time
from datetime import timedelta

# 1. ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': pendulum.datetime(2023, 1, 1, tz="Asia/Seoul"),
    'catchup': False,
    'execution_timeout': timedelta(hours=5) 
}

# DB ì—°ê²°ì„ ë•ëŠ” í•¨ìˆ˜ (ì¬ì—°ê²°ì„ ìœ„í•´ ë¶„ë¦¬)
def get_oracle_conn(conn_info):
    service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
    dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
    conn = oracledb.connect(
        user=conn_info.login,
        password=conn_info.password,
        dsn=dsn
    )
    return conn

def load_parquet_to_oracle(**kwargs):
    # ---------------------------------------------------------
    # 1. ì´ˆê¸° ì—°ê²° ì •ë³´ ì¤€ë¹„
    # ---------------------------------------------------------
    oracle_hook = OracleHook(oracle_conn_id='oracle_conn')
    conn_info = oracle_hook.get_connection('oracle_conn')
    
    # ìµœì´ˆ ì—°ê²°
    conn = get_oracle_conn(conn_info)
    cursor = conn.cursor()
    print("1. Oracle ìµœì´ˆ ì—°ê²° ì„±ê³µ")

    # ---------------------------------------------------------
    # 2. MinIO ìŠ¤íŠ¸ë¦¬ë° ì¤€ë¹„
    # ---------------------------------------------------------
    year = '2023'
    month = '01'
    bucket_name = 'bronze'
    file_key = f'taxi/year={year}/month={month}/yellow_tripdata_{year}-{month}.parquet'
    
    s3_hook = S3Hook(aws_conn_id='minio_conn')
    file_obj = s3_hook.get_key(key=file_key, bucket_name=bucket_name)
    
    if not file_obj:
        raise Exception("íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")

    data_stream = io.BytesIO(file_obj.get()['Body'].read())
    parquet_file = pq.ParquetFile(data_stream)
    
    # ---------------------------------------------------------
    # 3. ìŠ¤íŠ¸ë¦¬ë° ì ì¬ (ì¬ì—°ê²° ì „ëµ)
    # ---------------------------------------------------------
    target_columns = [
        'VENDOR_ID', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME', 
        'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATE_CODE_ID', 
        'STORE_AND_FWD_FLAG', 'PULOCATION_ID', 'DOLOCATION_ID', 
        'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX', 
        'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE', 
        'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE'
    ]
    
    insert_sql = f"""
    INSERT INTO TAXI_DATA ({', '.join(target_columns)}) 
    VALUES ({', '.join([':' + str(i+1) for i in range(len(target_columns))])})
    """

    BATCH_SIZE = 2000     # 2000ê±´ì”© ì ì¬
    RECONNECT_SIZE = 50000 # 5ë§Œ ê±´ë§ˆë‹¤ DB ì¬ì—°ê²° (ë©”ëª¨ë¦¬ ì´ˆê¸°í™”)
    total_count = 0
    
    print(f"3. ì ì¬ ì‹œì‘ (Batch: {BATCH_SIZE}, Reconnect: {RECONNECT_SIZE})")

    for i, batch in enumerate(parquet_file.iter_batches(batch_size=BATCH_SIZE)):
        try:
            # 1. Pandas ë³€í™˜
            df_chunk = batch.to_pandas()
            
            # 2. ì „ì²˜ë¦¬
            df_chunk = df_chunk.rename(columns={
                'VendorID': 'VENDOR_ID',
                'tpep_pickup_datetime': 'TPEP_PICKUP_DATETIME',
                'tpep_dropoff_datetime': 'TPEP_DROPOFF_DATETIME',
                'passenger_count': 'PASSENGER_COUNT',
                'trip_distance': 'TRIP_DISTANCE',
                'RatecodeID': 'RATE_CODE_ID',
                'store_and_fwd_flag': 'STORE_AND_FWD_FLAG',
                'PULocationID': 'PULOCATION_ID',
                'DOLocationID': 'DOLOCATION_ID',
                'payment_type': 'PAYMENT_TYPE',
                'fare_amount': 'FARE_AMOUNT',
                'extra': 'EXTRA',
                'mta_tax': 'MTA_TAX',
                'tip_amount': 'TIP_AMOUNT',
                'tolls_amount': 'TOLLS_AMOUNT',
                'improvement_surcharge': 'IMPROVEMENT_SURCHARGE',
                'total_amount': 'TOTAL_AMOUNT',
                'congestion_surcharge': 'CONGESTION_SURCHARGE',
                'airport_fee': 'AIRPORT_FEE'
            })
            
            for col in target_columns:
                if col not in df_chunk.columns:
                    df_chunk[col] = None
            
            df_chunk = df_chunk[target_columns].fillna(0)
            rows = [tuple(x) for x in df_chunk.to_numpy()]
            
            # 3. DB ì ì¬
            cursor.executemany(insert_sql, rows)
            conn.commit()
            
            total_count += len(rows)
            
            # 4. ë©”ëª¨ë¦¬ ì²­ì†Œ
            del df_chunk
            del rows
            gc.collect()

            # [í•µì‹¬ ì „ëµ] 5ë§Œ ê±´ë§ˆë‹¤ DB ì—°ê²° ëŠê³  ë‹¤ì‹œ ë§ºê¸°
            if total_count % RECONNECT_SIZE == 0:
                print(f"   ğŸ”„ [Clean-up] {total_count}ê±´ ë‹¬ì„±. DB ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
                cursor.close()
                conn.close()
                time.sleep(1) # 1ì´ˆê°„ ì™„ë²½í•˜ê²Œ ì—°ê²° í•´ì œ ëŒ€ê¸°
                
                # ë‹¤ì‹œ ì—°ê²°
                conn = get_oracle_conn(conn_info)
                cursor = conn.cursor()
                print(f"   âœ… [Resumed] DB ì¬ì—°ê²° ì™„ë£Œ.")
            
            # ë¡œê·¸ëŠ” 10ë§Œ ê±´ë§ˆë‹¤ í•œ ë²ˆë§Œ ì¶œë ¥ (ë¡œê·¸ ë¶€ë‹´ ìµœì†Œí™”)
            elif total_count % 100000 == 0:
                 print(f"   -> ëˆ„ì  {total_count} ê±´ ì ì¬ ì§„í–‰ ì¤‘...")

            # 0.01ì´ˆ íœ´ì‹ (ì¬ì—°ê²° ì „ëµì´ ìˆìœ¼ë¯€ë¡œ íœ´ì‹ì€ ì§§ê²Œ)
            time.sleep(0.01)

        except Exception as e:
            print(f"   -> [Chunk {i+1}] ì—ëŸ¬ ë°œìƒ: {e}")
            raise e

    cursor.close()
    conn.close()
    print(f"âœ… ì´ {total_count} ê±´ ì ì¬ ì™„ë£Œ!")

with DAG(
    dag_id='03_minio_to_oracle',
    default_args=default_args,
    schedule=None,
    tags=['portfolio', 'oracle', 'elt', 'optimized'],
) as dag:

    load_task = PythonOperator(
        task_id='load_to_oracle',
        python_callable=load_parquet_to_oracle
    )