from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.oracle.hooks.oracle import OracleHook
import pandas as pd
import pendulum
import io
import oracledb
import pyarrow.parquet as pq
import gc
import time
from datetime import timedelta

# 1. ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': pendulum.datetime(2023, 1, 1, tz="Asia/Seoul"),
    'catchup': False,
    'execution_timeout': timedelta(hours=5) 
}

def get_oracle_conn(conn_info):
    service_name = conn_info.schema if conn_info.schema else 'Oracle23ai'
    dsn = f"{conn_info.host}:{conn_info.port}/{service_name}"
    conn = oracledb.connect(
        user=conn_info.login,
        password=conn_info.password,
        dsn=dsn
    )
    return conn

def load_parquet_to_oracle(**kwargs):
    oracle_hook = OracleHook(oracle_conn_id='oracle_conn')
    conn_info = oracle_hook.get_connection('oracle_conn')
    
    conn = get_oracle_conn(conn_info)
    cursor = conn.cursor()
    print("1. Oracle ìµœì´ˆ ì—°ê²° ì„±ê³µ")

    # ---------------------------------------------------------
    # MinIO ì—°ê²°
    # ---------------------------------------------------------
    year = '2023'
    month = '01'
    bucket_name = 'bronze'
    file_key = f'taxi/year={year}/month={month}/yellow_tripdata_{year}-{month}.parquet'
    
    s3_hook = S3Hook(aws_conn_id='minio_conn')
    file_obj = s3_hook.get_key(key=file_key, bucket_name=bucket_name)
    
    if not file_obj:
        raise Exception("íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")

    data_stream = io.BytesIO(file_obj.get()['Body'].read())
    parquet_file = pq.ParquetFile(data_stream)
    
    # ---------------------------------------------------------
    # ì»¬ëŸ¼ ì •ì˜
    # ---------------------------------------------------------
    target_columns = [
        'VENDOR_ID', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME', 
        'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATE_CODE_ID', 
        'STORE_AND_FWD_FLAG', 'PULOCATION_ID', 'DOLOCATION_ID', 
        'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX', 
        'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE', 
        'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE'
    ]
    
    # [ìˆ˜ì •] ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•´ì•¼ í•  ì»¬ëŸ¼ ì§€ì • (ì´ ì»¬ëŸ¼ë“¤ì€ 0 ëŒ€ì‹  'N'ì´ë‚˜ ë¹ˆì¹¸ìœ¼ë¡œ ì±„ì›€)
    string_columns = ['STORE_AND_FWD_FLAG', 'VENDOR_ID'] 

    insert_sql = f"""
    INSERT INTO TAXI_DATA ({', '.join(target_columns)}) 
    VALUES ({', '.join([':' + str(i+1) for i in range(len(target_columns))])})
    """

    BATCH_SIZE = 2000
    RECONNECT_SIZE = 50000 
    total_count = 0
    
    print(f"3. ì ì¬ ì‹œì‘ (Batch: {BATCH_SIZE}, Reconnect: {RECONNECT_SIZE})")

    for i, batch in enumerate(parquet_file.iter_batches(batch_size=BATCH_SIZE)):
        try:
            df_chunk = batch.to_pandas()
            
            # ì»¬ëŸ¼ ì´ë¦„ ë§¤í•‘
            df_chunk = df_chunk.rename(columns={
                'VendorID': 'VENDOR_ID',
                'tpep_pickup_datetime': 'TPEP_PICKUP_DATETIME',
                'tpep_dropoff_datetime': 'TPEP_DROPOFF_DATETIME',
                'passenger_count': 'PASSENGER_COUNT',
                'trip_distance': 'TRIP_DISTANCE',
                'RatecodeID': 'RATE_CODE_ID',
                'store_and_fwd_flag': 'STORE_AND_FWD_FLAG',
                'PULocationID': 'PULOCATION_ID',
                'DOLocationID': 'DOLOCATION_ID',
                'payment_type': 'PAYMENT_TYPE',
                'fare_amount': 'FARE_AMOUNT',
                'extra': 'EXTRA',
                'mta_tax': 'MTA_TAX',
                'tip_amount': 'TIP_AMOUNT',
                'tolls_amount': 'TOLLS_AMOUNT',
                'improvement_surcharge': 'IMPROVEMENT_SURCHARGE',
                'total_amount': 'TOTAL_AMOUNT',
                'congestion_surcharge': 'CONGESTION_SURCHARGE',
                'airport_fee': 'AIRPORT_FEE'
            })
            
            # ì—†ëŠ” ì»¬ëŸ¼ ìƒì„±
            for col in target_columns:
                if col not in df_chunk.columns:
                    df_chunk[col] = None
            
            # [í•µì‹¬ ìˆ˜ì •] ë¬¸ìì—´ ì»¬ëŸ¼ì€ 'N'ìœ¼ë¡œ ì±„ìš°ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì±„ì›€
            # ì´ë ‡ê²Œ í•´ì•¼ VARCHAR ì»¬ëŸ¼ì— int(0)ì´ ë“¤ì–´ê°€ëŠ” ì—ëŸ¬ë¥¼ ë§‰ìŒ
            for str_col in string_columns:
                if str_col in df_chunk.columns:
                    # ë¹ˆ ê°’ì„ ë¬¸ìì—´ 'N'ìœ¼ë¡œ ì±„ìš°ê³ , ê°•ì œë¡œ ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    df_chunk[str_col] = df_chunk[str_col].fillna('N').astype(str)
            
            # ë‚˜ë¨¸ì§€ ìˆ«ìí˜• ì»¬ëŸ¼ë“¤ì˜ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
            df_chunk = df_chunk.fillna(0)

            # ë°ì´í„° ì¤€ë¹„
            df_chunk = df_chunk[target_columns]
            rows = [tuple(x) for x in df_chunk.to_numpy()]
            
            cursor.executemany(insert_sql, rows)
            conn.commit()
            
            total_count += len(rows)
            
            del df_chunk
            del rows
            gc.collect()

            if total_count % RECONNECT_SIZE == 0:
                print(f"   ğŸ”„ [Clean-up] {total_count}ê±´ ë‹¬ì„±. DB ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
                cursor.close()
                conn.close()
                time.sleep(1)
                conn = get_oracle_conn(conn_info)
                cursor = conn.cursor()
                print(f"   âœ… [Resumed] DB ì¬ì—°ê²° ì™„ë£Œ.")
            
            elif total_count % 100000 == 0:
                 print(f"   -> ëˆ„ì  {total_count} ê±´ ì ì¬ ì§„í–‰ ì¤‘...")

            time.sleep(0.01)

        except Exception as e:
            print(f"   -> [Chunk {i+1}] ì—ëŸ¬ ë°œìƒ: {e}")
            raise e

    cursor.close()
    conn.close()
    print(f"âœ… ì´ {total_count} ê±´ ì ì¬ ì™„ë£Œ!")

with DAG(
    dag_id='03_minio_to_oracle',
    default_args=default_args,
    schedule=None,
    tags=['portfolio', 'oracle', 'elt', 'optimized'],
) as dag:

    load_task = PythonOperator(
        task_id='load_to_oracle',
        python_callable=load_parquet_to_oracle
    )