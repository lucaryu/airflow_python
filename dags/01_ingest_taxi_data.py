from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import requests
import pendulum
import io

# 1. DAG ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': pendulum.datetime(2024, 1, 1, tz="Asia/Seoul"),
    'catchup': False,
}

# 2. í•¨ìˆ˜ ì •ì˜
def download_and_upload_to_minio():
    # NYC Taxi ë°ì´í„° (2023ë…„ 1ì›” Yellow Taxi)
    url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
    filename = "yellow_tripdata_2023-01.parquet"
    bucket_name = "bronze"
    key = f"taxi/year=2023/month=01/{filename}"

    print(f"ë‹¤ìš´ë¡œë“œ ì‹œìž‘: {url}")
    
    # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    # Airflow Connection ID ì‚¬ìš©
    s3_hook = S3Hook(aws_conn_id='minio_conn')
    
    # ë²„í‚· ì—†ìœ¼ë©´ ìƒì„±
    if not s3_hook.check_for_bucket(bucket_name):
        s3_hook.create_bucket(bucket_name)
    
    # ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ ì—…ë¡œë“œ
    file_obj = io.BytesIO(response.content)
    s3_hook.load_file_obj(
        file_obj=file_obj,
        key=key,
        bucket_name=bucket_name,
        replace=True
    )
    
    print(f"ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket_name}/{key}")

# 3. DAG ì •ì˜
with DAG(
    dag_id='01_ingest_taxi_data',
    default_args=default_args,
    schedule=None,  # ðŸ‘ˆ ì—¬ê¸°ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤! (schedule_interval -> schedule)
    tags=['portfolio', 'ingestion'],
) as dag:

    task_upload = PythonOperator(
        task_id='upload_to_minio',
        python_callable=download_and_upload_to_minio
    )